{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SignVisionAiGTSRB (German Traffic Sign Recognition)\n",
        "\n",
        "Komplettes Notebook zum Trainieren, Evaluieren und Anwenden eines CNN‑Modells für deutsche Verkehrsschilder (GTSRB) auf Basis eines Kaggle‑Datasets.\n",
        "\n",
        "**Funktionen**\n",
        "- Kaggle‑Download (mit `kaggle.json`)\n",
        "- Flexible Dateneinbindung: CSV‑basiert **oder** Ordnerstruktur\n",
        "- Train/Val‑Split, Augmentierung, Klassenlisten\n",
        "- CNN‑Training (Keras/TensorFlow) mit Checkpoints & EarlyStopping\n",
        "- Auswertung: Accuracy-/Loss‑Plots, Confusion‑Matrizen, Klassifikationsbericht\n",
        "- Inferenz: Einzelbild‑Vorhersage + Top‑5‑Balken, Webcam‑Snapshot (Colab)\n",
        "- Speichern & Laden des `.keras`‑Modells\n",
        "\n",
        "**Voraussetzungen**\n",
        "- Laufzeit: Google Colab (oder lokale Umgebung, **GPU empfohlen**)\n",
        "- `kaggle.json` (API‑Key) verfügbar  \n",
        "- Python 3.10+, TensorFlow 2.x, Plotly, OpenCV\n",
        "\n",
        "> Hinweis: Die Datei `kaggle.json` wird von diesem Notebook automatisch nach `~/.kaggle/kaggle.json` kopiert.\n",
        "\n",
        "**Hinweis zu Reproduzierbarkeit**\n",
        "Seeds werden gesetzt (NumPy/TensorFlow); Ergebnisse können je nach Hardware/Laufzeit leicht variieren.\n",
        "\n",
        "**Datenquelle & Lizenz**\n",
        "GTSRB (Kaggle‑Mirror). Es gelten die Lizenzhinweise des jeweiligen Kaggle‑Datasets.\n"
      ],
      "metadata": {
        "id": "wYTW2N8q00Nf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5509_JNmZX_"
      },
      "outputs": [],
      "source": [
        "!pip install -q plotly opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 0) Umgebung & Variablen - Bibliotheken importieren"
      ],
      "metadata": {
        "id": "pJHULN0M1ARe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In dieser Zelle werden alle notwendigen Bibliotheken\n",
        "# für das Projekt geladen. Die Struktur ist nach\n",
        "# Anwendungsbereichen sortiert.\n",
        "\n",
        "# --- Standardbibliothek (Allgemeine Tools) ---\n",
        "import os, sys, shutil, json, zipfile, glob, random, itertools\n",
        "from pathlib import Path\n",
        "from math import ceil\n",
        "\n",
        "# --- Numerik / Datenanalyse ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Machine Learning / Deep Learning ---\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# --- Visualisierung ---\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# --- Colab-spezifisches I/O & Computer Vision (nur falls nötig) ---\n",
        "from base64 import b64decode\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab import output   # nur in Google Colab verfügbar\n",
        "import cv2"
      ],
      "metadata": {
        "id": "KGVk2-VAwLhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Projekt-Konfiguration (Pfade, Dataset, Parameter)"
      ],
      "metadata": {
        "id": "SkM5uQFW1IFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In dieser Zelle werden die grundlegenden Projektpfade,\n",
        "# Kaggle-Dataset-Slug sowie die Trainingsparameter definiert.\n",
        "\n",
        "# --- Verzeichnisstruktur ---\n",
        "PROJECT_ROOT = Path.cwd()                  # Projektwurzel = aktuelles Arbeitsverzeichnis\n",
        "DATA_ROOT = PROJECT_ROOT / \"data_gtsrb\"    # Hauptordner für alle Daten\n",
        "RAW_DIR = DATA_ROOT / \"raw\"                # Rohdaten\n",
        "EXTRACT_DIR = DATA_ROOT / \"extracted\"      # Entpackte Daten\n",
        "WORK_DIR = DATA_ROOT / \"work\"              # Arbeitsverzeichnis\n",
        "MODELS_DIR = PROJECT_ROOT / \"models\"       # Modell-Speicherort\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)  # Ordner anlegen, falls nicht vorhanden\n",
        "\n",
        "# --- Kaggle Dataset-Slug ---\n",
        "# Beispiele für GTSRB:\n",
        "#   - 'meowmeowmeowmeowmeow/gtsrb-german-traffic-sign'\n",
        "#   - 'valentynsichkar/traffic-signs-preprocessed'\n",
        "#   - 'hgyemm/gtsrb-german-traffic-signs'\n",
        "#   - 'jithinjosepk1/gtsrb-german-traffic-sign-classification'\n",
        "KAGGLE_DATASET = os.environ.get('KAGGLE_DATASET','meowmeowmeowmeowmeow/gtsrb-german-traffic-sign')\n",
        "\n",
        "# --- Trainingsparameter ---\n",
        "IMG_HEIGHT = 48          # Höhe der Input-Bilder\n",
        "IMG_WIDTH = 48           # Breite der Input-Bilder\n",
        "BATCH_SIZE = 64          # Batchgröße für Training\n",
        "EPOCHS = 20              # Anzahl Trainings-Epochen\n",
        "VAL_SPLIT = 0.15         # Anteil der Validierungsdaten\n",
        "SEED = 42                # Zufallssamen für Reproduzierbarkeit\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Rückgabe zur Kontrolle\n",
        "DATA_ROOT, RAW_DIR, EXTRACT_DIR, WORK_DIR\n"
      ],
      "metadata": {
        "id": "M24Y1vkiwLgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Kaggle-CLI installieren"
      ],
      "metadata": {
        "id": "30wvXXghM5he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mit diesem Befehl wird die offizielle Kaggle-CLI\n",
        "# (Command Line Interface) installiert.\n",
        "# Sie wird benötigt, um später den Datensatz von Kaggle\n",
        "# herunterzuladen und mit dem Account zu authentifizieren.\n",
        "\n",
        "!pip -q install kaggleol'"
      ],
      "metadata": {
        "id": "vvuV1RDqwrVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Kaggle-API Schlüsseldatei hochladen"
      ],
      "metadata": {
        "id": "jB5o1nQMNRde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) kaggle.json hochladen (Dateidialog -> wähle deine lokale /home/dan/Documents/Colab/kaggle.json)\n",
        "from google.colab import files\n",
        "files.upload()   # ⇦ Datei-Dialog öffnet sich – wähle hier deine lokale kaggle.json"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GWQyd_EDwzGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Zusätzliche Bibliotheken installieren"
      ],
      "metadata": {
        "id": "KAK5UNh_Nh53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Einige Pakete sind in Colab nicht standardmäßig enthalten\n",
        "# und werden hier nachinstalliert:\n",
        "# - plotly:   Interaktive Visualisierung\n",
        "# - opencv-python (cv2): Bildverarbeitung / Computer Vision\n",
        "\n",
        "!pip install -q plotly opencv-python"
      ],
      "metadata": {
        "id": "z93eMjzKmqde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Kaggle-API Schlüsseldatei einrichten"
      ],
      "metadata": {
        "id": "q0wdGRxzOS9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Die hochgeladene Schlüsseldatei (kaggle.json) wird in das\n",
        "# Standardverzeichnis ~/.kaggle verschoben.\n",
        "# Zusätzlich werden die Zugriffsrechte so gesetzt,\n",
        "# dass nur der Besitzer die Datei lesen darf.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# Damit ist die Kaggle-Authentifizierung abgeschlossen."
      ],
      "metadata": {
        "id": "D0RQ4OEQw4L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) (Optional) Dataset-Slug suchen und prüfen"
      ],
      "metadata": {
        "id": "6lePfzdBPJKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mit diesem Befehl werden verfügbare Kaggle-Datasets gelistet,\n",
        "# die zum Suchbegriff passen.\n",
        "# Damit kann überprüft werden, welcher Slug für den Download\n",
        "# verwendet werden soll.\n",
        "# (Die Ausgabe zeigt u.a. Referenz, Titel, Größe, Datum)\n",
        "\n",
        "!kaggle datasets list -s \"gtsrb german traffic sign\" | head -n 20"
      ],
      "metadata": {
        "id": "Y_9cJFudw-5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Datensatz herunterladen und entpacken"
      ],
      "metadata": {
        "id": "EzQNxcdzPdlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In dieser Zelle wird der ausgewählte GTSRB-Datensatz\n",
        "# von Kaggle heruntergeladen, entpackt und die Verzeichnisstruktur\n",
        "# zur Kontrolle ausgegeben.\n",
        "\n",
        "# Kaggle-Dataset-Slug (falls nötig anpassen)\n",
        "KAGGLE_DATASET = \"meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\"\n",
        "\n",
        "# Verzeichnisse für Rohdaten und entpackte Daten\n",
        "RAW_DIR = \"data_gtsrb/raw\"\n",
        "EXTRACT_DIR = \"data_gtsrb/extracted\"\n",
        "!mkdir -p {RAW_DIR} {EXTRACT_DIR}\n",
        "\n",
        "# Download des Datensatzes ins RAW_DIR\n",
        "!kaggle datasets download -d {KAGGLE_DATASET} -p {RAW_DIR} --force\n",
        "\n",
        "# Entpacken in EXTRACT_DIR\n",
        "!unzip -q -o {RAW_DIR}/*.zip -d {EXTRACT_DIR}\n",
        "\n",
        "# Verzeichnisstruktur (erste 20 Treffer) anzeigen\n",
        "!find {EXTRACT_DIR} -maxdepth 2 -type d -print | head -n 20\n",
        "\n",
        "# Info: Kaggle-Dataset-URL\n",
        "print(\"Dataset URL:\", f\"https://www.kaggle.com/datasets/{KAGGLE_DATASET}\")"
      ],
      "metadata": {
        "id": "ApXJgYNDmg1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Datensatz laden (CSV- oder Ordner-basiert)"
      ],
      "metadata": {
        "id": "kb99ij1kQWyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Zelle implementiert zwei Varianten zum Laden des GTSRB-Datensatzes:\n",
        "# - load_data_csv():    für Datensätze, die CSV-Dateien mit Pfadangaben enthalten\n",
        "# - load_data_dirs():   für Datensätze, die als Ordnerstruktur vorliegen\n",
        "#\n",
        "# Abhängig von der Einstellung IS_CSV wird die passende Methode aufgerufen.\n",
        "# Das Ergebnis (data_info) enthält Mappings, Splits und Pfade.\n",
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 0) Basis-Pfade robust setzen (bei Bedarf anpassen)\n",
        "#    -> Wenn dein Datensatz woanders liegt: BASE_DIR = Path(\"/pfad/zu/deinem/gtsrb\")\n",
        "# -----------------------------------------------------------\n",
        "BASE_DIR = Path(globals().get(\"BASE_DIR\", \".\"))  # ggf. anpassen\n",
        "EXTRACT_DIR = Path(globals().get(\"EXTRACT_DIR\", BASE_DIR))\n",
        "\n",
        "# CSV-/Dir-Modus automatisch erkennen, falls IS_CSV nicht gesetzt ist\n",
        "try:\n",
        "    IS_CSV\n",
        "except NameError:\n",
        "    IS_CSV = any([\n",
        "        (BASE_DIR / \"Train.csv\").exists(),\n",
        "        (BASE_DIR / \"train.csv\").exists(),\n",
        "        (EXTRACT_DIR / \"Train.csv\").exists(),\n",
        "        (EXTRACT_DIR / \"train.csv\").exists(),\n",
        "    ])\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1) Fallback: build_classmap_from_dirs, falls noch nicht definiert\n",
        "#    Erwartet Ordnerstruktur mit Unterordnern pro Klasse\n",
        "# -----------------------------------------------------------\n",
        "try:\n",
        "    build_classmap_from_dirs\n",
        "except NameError:\n",
        "    def build_classmap_from_dirs(root: Path):\n",
        "        # train-Kandidaten: root, root/Train, root/train\n",
        "        for cand in [root, root / \"Train\", root / \"train\"]:\n",
        "            if not cand.exists():\n",
        "                continue\n",
        "            subdirs = [d for d in cand.iterdir() if d.is_dir()]\n",
        "            classes = [d.name for d in subdirs if any(d.rglob(\"*.png\")) or any(d.rglob(\"*.jpg\")) or any(d.rglob(\"*.jpeg\"))]\n",
        "            classes = sorted(set(classes))\n",
        "            if classes:\n",
        "                class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "                return class_to_idx, cand\n",
        "        raise FileNotFoundError(\n",
        "            f\"Keine Klassen-Unterordner mit Bildern unter {root} gefunden \"\n",
        "            \"(erwartet z.B. Train/00000, Train/00001, …).\"\n",
        "        )\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2) CSV-Loader\n",
        "# -----------------------------------------------------------\n",
        "def load_data_csv(base_dir: Path):\n",
        "    train_csv = base_dir / \"Train.csv\"\n",
        "    if not train_csv.exists():\n",
        "        alt = base_dir / \"train.csv\"\n",
        "        if alt.exists():\n",
        "            train_csv = alt\n",
        "    test_csv = base_dir / \"Test.csv\"\n",
        "\n",
        "    if not train_csv.exists():\n",
        "        raise FileNotFoundError(f\"Train.csv wurde in {base_dir} nicht gefunden.\")\n",
        "\n",
        "    df_train = pd.read_csv(train_csv)\n",
        "    df_test = pd.read_csv(test_csv) if test_csv.exists() else None\n",
        "\n",
        "    # Pfadspalte finden\n",
        "    path_col = None\n",
        "    for cand in [\"Path\", \"Filename\", \"ImagePath\", \"img\", \"image\", \"file\"]:\n",
        "        if cand in df_train.columns:\n",
        "            path_col = cand\n",
        "            break\n",
        "    if path_col is None:\n",
        "        raise ValueError(\"Keine gültige Pfad-Spalte gefunden (z.B. 'Path' oder 'Filename').\")\n",
        "\n",
        "    # Klassen-Spalte normalisieren -> 'ClassId'\n",
        "    if \"ClassId\" not in df_train.columns:\n",
        "        for cand in [\"classId\", \"label\", \"Class\", \"Category\"]:\n",
        "            if cand in df_train.columns:\n",
        "                df_train = df_train.rename(columns={cand: \"ClassId\"})\n",
        "                break\n",
        "    if \"ClassId\" not in df_train.columns:\n",
        "        raise ValueError(\"Keine Klassen-Spalte gefunden (erwartet: 'ClassId').\")\n",
        "\n",
        "    # Pfade auflösen (relativ -> absolut)\n",
        "    def mkpath(p):\n",
        "        p = str(p)\n",
        "        # direkt\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "        # unter Basis\n",
        "        cand = base_dir / p\n",
        "        if cand.exists():\n",
        "            return str(cand)\n",
        "        # typische Unterordner probieren\n",
        "        for sub in [\"Train\", \"train\", \"images\", \"Images\", \"GTSRB\", \"GTSRB/Train\"]:\n",
        "            cand = base_dir / sub / p\n",
        "            if cand.exists():\n",
        "                return str(cand)\n",
        "        return str((base_dir / p).as_posix())\n",
        "\n",
        "    df_train[\"filepath\"] = df_train[path_col].astype(str).apply(mkpath)\n",
        "    df_train = df_train[df_train[\"filepath\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "\n",
        "    if df_test is not None and path_col in df_test.columns:\n",
        "        df_test[\"filepath\"] = df_test[path_col].astype(str).apply(mkpath)\n",
        "        df_test = df_test[df_test[\"filepath\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "\n",
        "    # Klassen-Mapping\n",
        "    classes = sorted(df_train[\"ClassId\"].unique())\n",
        "    idx_to_class = {i: c for i, c in enumerate(classes)}\n",
        "    class_to_idx = {c: i for i, c in idx_to_class.items()}\n",
        "\n",
        "    # Labels auf 0..N-1 mappen\n",
        "    df_train[\"label\"] = df_train[\"ClassId\"].map(class_to_idx)\n",
        "    if df_test is not None and \"ClassId\" in df_test.columns:\n",
        "        df_test[\"label\"] = df_test[\"ClassId\"].map(class_to_idx)\n",
        "\n",
        "    # Train/Val-Split (VAL_SPLIT/SEED müssen global gesetzt sein – sonst Defaults wählen)\n",
        "    val_split = globals().get(\"VAL_SPLIT\", 0.15)\n",
        "    seed = globals().get(\"SEED\", 42)\n",
        "    train_df, val_df = train_test_split(\n",
        "        df_train, test_size=val_split, random_state=seed, stratify=df_train[\"label\"]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"mode\": \"csv\",\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "        \"idx_to_class\": idx_to_class,\n",
        "        \"train_df\": train_df.reset_index(drop=True),\n",
        "        \"val_df\":   val_df.reset_index(drop=True),\n",
        "        \"test_df\":  None if df_test is None else df_test.reset_index(drop=True),\n",
        "    }\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3) Ordner-Loader\n",
        "# -----------------------------------------------------------\n",
        "def load_data_dirs(extract_dir: Path):\n",
        "    class_to_idx, train_base = build_classmap_from_dirs(extract_dir)\n",
        "\n",
        "    # Test/Val-Basis optional erkennen\n",
        "    test_base = None\n",
        "    for name in [\"test\", \"Test\", \"testing\", \"Testing\", \"val\", \"Val\", \"validation\"]:\n",
        "        cand = extract_dir / name\n",
        "        if cand.exists():\n",
        "            test_base = cand\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        \"mode\": \"dirs\",\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "        \"idx_to_class\": {i: c for c, i in class_to_idx.items()},\n",
        "        \"train_base\": train_base,\n",
        "        \"test_base\": test_base,\n",
        "    }\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4) Hauptlogik: CSV oder Ordner\n",
        "# -----------------------------------------------------------\n",
        "if IS_CSV:\n",
        "    data_info = load_data_csv(BASE_DIR if (BASE_DIR / \"Train.csv\").exists() or (BASE_DIR / \"train.csv\").exists()\n",
        "                              else EXTRACT_DIR)\n",
        "else:\n",
        "    data_info = load_data_dirs(EXTRACT_DIR)\n",
        "\n",
        "# Kurze Übersicht\n",
        "print(\"Modus:\", data_info[\"mode\"])\n",
        "print(\"Klassen (Beispiel):\", list(data_info.get(\"class_to_idx\", {}).items())[:5])\n"
      ],
      "metadata": {
        "id": "MVrTpoVDzy3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) GTSRB-Klassen (43 Labels) und Hilfsfunktionen"
      ],
      "metadata": {
        "id": "Y7hcM3AxQ-yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Zelle definiert die vollständige Liste der 43 Klassen\n",
        "# (German Traffic Sign Recognition Benchmark).\n",
        "# Zusätzlich gibt es eine Funktion, die ein Label (Index)\n",
        "# in den passenden Klassennamen umwandelt.\n",
        "\n",
        "# Klassen-Namen (Index entspricht Label-ID)\n",
        "CLASS_NAMES = [\n",
        "    \"Speed limit (20km/h)\", \"Speed limit (30km/h)\", \"Speed limit (50km/h)\",\n",
        "    \"Speed limit (60km/h)\", \"Speed limit (70km/h)\", \"Speed limit (80km/h)\",\n",
        "    \"End of speed limit (80km/h)\", \"Speed limit (100km/h)\", \"Speed limit (120km/h)\",\n",
        "    \"No passing\", \"No passing >3.5t\", \"Right of way at next intersection\",\n",
        "    \"Priority road\", \"Yield\", \"Stop\", \"No vehicles\", \"No trucks (>3.5t)\", \"No entry\",\n",
        "    \"General caution\", \"Dangerous curve left\", \"Dangerous curve right\", \"Double curve\",\n",
        "    \"Bumpy road\", \"Slippery road\", \"Road narrows (right)\", \"Road work\", \"Traffic signals\",\n",
        "    \"Pedestrians\", \"Children crossing\", \"Bicycles crossing\", \"Beware of ice/snow\",\n",
        "    \"Wild animals crossing\", \"End of all speed/passing limits\", \"Turn right ahead\",\n",
        "    \"Turn left ahead\", \"Ahead only\", \"Go straight or right\", \"Go straight or left\",\n",
        "    \"Keep right\", \"Keep left\", \"Roundabout mandatory\", \"End of no passing\",\n",
        "    \"End of no passing >3.5t\"\n",
        "]\n",
        "\n",
        "# Hilfsfunktion: Label -> Name\n",
        "def label_to_name(i: int) -> str:\n",
        "    return CLASS_NAMES[int(i)] if 0 <= int(i) < len(CLASS_NAMES) else f\"class_{int(i)}\"\n",
        "\n",
        "# Übersicht der Klassen\n",
        "for idx, name in enumerate(CLASS_NAMES):\n",
        "    print(f\"{idx:2d}: {name}\")\n"
      ],
      "metadata": {
        "id": "O7UmW0v6xgJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) tf.data-Pipelines für Training und Validierung"
      ],
      "metadata": {
        "id": "FwWG1uDg12T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datensätze vorbereiten (CSV- oder Ordner-Variante)\n",
        "# Definiert Hilfsfunktionen und baut train_ds / val_ds auf.\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Hilfsfunktion: Einzelnes Bild laden und normalisieren\n",
        "# ------------------------------------------------\n",
        "def decode_img(path):\n",
        "    img = tf.io.read_file(path)                                             # Datei einlesen\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)   # PNG/JPG -> Tensor\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])                     # auf Zielgröße skalieren\n",
        "    img = tf.cast(img, tf.float32) / 255.0                                  # Normalisierung 0..1\n",
        "    return img\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Datensatz aus DataFrame erstellen (CSV-Variante)\n",
        "# ------------------------------------------------\n",
        "def make_dataset_from_df(df, shuffle=True):\n",
        "    paths = df['filepath'].astype(str).values      # Pfade als Strings\n",
        "    labels = df['label'].values\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(df), seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda p, l: (decode_img(p), tf.cast(l, tf.int32)),\n",
        "                num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Datensatz aus Ordnerstruktur erstellen (Dir-Variante)\n",
        "# ------------------------------------------------\n",
        "def make_dataset_from_dirs(base_dir: Path, class_to_idx: dict, shuffle=True):\n",
        "    samples = []\n",
        "    for cls, idx in class_to_idx.items():\n",
        "        folder = base_dir/cls\n",
        "        if folder.exists():\n",
        "            for p in folder.rglob(\"*.png\"):\n",
        "                samples.append((str(p), idx))\n",
        "            for p in folder.rglob(\"*.jpg\"):\n",
        "                samples.append((str(p), idx))\n",
        "\n",
        "    # DataFrame erzeugen\n",
        "    paths = [s[0] for s in samples]\n",
        "    labels = [s[1] for s in samples]\n",
        "    df = pd.DataFrame({'filepath': paths, 'label': labels})\n",
        "\n",
        "    # Shuffle (optional)\n",
        "    if shuffle and len(df) > 0:\n",
        "        df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "    # Train/Val-Split\n",
        "    if len(df) > 0:\n",
        "        train_df, val_df = train_test_split(\n",
        "            df, test_size=VAL_SPLIT, random_state=SEED, stratify=df['label']\n",
        "        )\n",
        "    else:\n",
        "        train_df, val_df = df, df\n",
        "\n",
        "    return make_dataset_from_df(train_df), make_dataset_from_df(val_df, shuffle=False), train_df, val_df\n",
        "\n",
        "# ------------------------------------------------\n",
        "# data_info sicherstellen (falls oben nicht erzeugt)\n",
        "# ------------------------------------------------\n",
        "try:\n",
        "    data_info\n",
        "except NameError:\n",
        "    # versuche CSV zu laden, sonst Ordnerstruktur\n",
        "    if (BASE_DIR/\"Train.csv\").exists() or (BASE_DIR/\"train.csv\").exists():\n",
        "        data_info = load_data_csv(BASE_DIR)\n",
        "    else:\n",
        "        data_info = load_data_dirs(BASE_DIR)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# Hauptlogik: Auswahl CSV- oder Dir-Variante\n",
        "# ------------------------------------------------\n",
        "if data_info['mode'] == 'csv':\n",
        "    # erwartet Spalten: filepath (string), label (remapped)\n",
        "    train_ds = make_dataset_from_df(data_info['train_df'])\n",
        "    val_ds   = make_dataset_from_df(data_info['val_df'], shuffle=False)\n",
        "    test_df  = data_info.get('test_df')\n",
        "    num_classes = len(data_info['class_to_idx'])\n",
        "else:\n",
        "    train_base = data_info['train_base']\n",
        "    train_ds, val_ds, train_df, val_df = make_dataset_from_dirs(\n",
        "        train_base, data_info['class_to_idx']\n",
        "    )\n",
        "    test_df = None\n",
        "    num_classes = len(data_info['class_to_idx'])\n",
        "\n",
        "# Kontrolle: Anzahl Klassen\n",
        "print(\"Klassenanzahl:\", num_classes)\n"
      ],
      "metadata": {
        "id": "thtDgadMwMA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) tf.data: Augmentierung, Datasets und Batch-Vorschau"
      ],
      "metadata": {
        "id": "qp5JQdhm2FGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Zelle ergänzt die Datenpipelines um Bild‑Augmentierung\n",
        "# (nur im Training) und erstellt Trainings-/Validierungs‑Datasets.\n",
        "# Anschließend wird ein Batch als Raster mit Textlabels visualisiert.\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Klassenliste sicherstellen (falls CLASS_NAMES nicht definiert ist)\n",
        "try:\n",
        "    CLASS_NAMES\n",
        "except NameError:\n",
        "    CLASS_NAMES = [data_info['idx_to_class'][i] for i in range(num_classes)]\n",
        "\n",
        "# --- Augmentierung (nur für Training) ---\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomRotation(0.05),\n",
        "        tf.keras.layers.RandomZoom(0.10),\n",
        "        tf.keras.layers.RandomContrast(0.10),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Hinweis: Horizontal-Flip ist für Verkehrszeichen meist ungeeignet.\n",
        "\n",
        "# Bild laden, skalieren, normalisieren\n",
        "def decode_img(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "# Dataset aus Pfad-/Label-Arrays bauen\n",
        "def make_dataset(paths, labels, training=True, batch_size=BATCH_SIZE):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(buffer_size=len(paths), seed=SEED, reshuffle_each_iteration=True)\n",
        "\n",
        "    def _map(p, y):\n",
        "        x = decode_img(p)\n",
        "        if training:\n",
        "            x = data_augmentation(x, training=True)\n",
        "        return x, tf.cast(y, tf.int32)\n",
        "\n",
        "    ds = ds.map(_map, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# --- Quelldaten aus CSV- oder Ordner-Modus ermitteln ---\n",
        "if data_info['mode'] == 'csv':\n",
        "    train_df = data_info['train_df']\n",
        "    val_df   = data_info['val_df']\n",
        "    x_tr = train_df['filepath'].to_numpy()\n",
        "    y_tr = train_df['label'].to_numpy()\n",
        "    x_va = val_df['filepath'].to_numpy()\n",
        "    y_va = val_df['label'].to_numpy()\n",
        "else:\n",
        "    # Dateien aus Ordnerstruktur sammeln\n",
        "    def gather(base: Path, class_to_idx: dict):\n",
        "        paths, labels = [], []\n",
        "        for cls, idx in class_to_idx.items():\n",
        "            folder = base / cls\n",
        "            if not folder.exists():\n",
        "                continue\n",
        "            for ext in (\"*.png\", \"*.jpg\", \"*.jpeg\"):\n",
        "                for p in folder.rglob(ext):\n",
        "                    paths.append(str(p))\n",
        "                    labels.append(idx)\n",
        "        return np.array(paths), np.array(labels)\n",
        "\n",
        "    train_base = data_info['train_base']\n",
        "    x_all, y_all = gather(train_base, data_info['class_to_idx'])\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        np.arange(len(y_all)), test_size=VAL_SPLIT, random_state=SEED, stratify=y_all\n",
        "    )\n",
        "    x_tr, y_tr = x_all[tr_idx], y_all[tr_idx]\n",
        "    x_va, y_va = x_all[va_idx], y_all[va_idx]\n",
        "\n",
        "# Basiskontrolle\n",
        "assert len(x_tr) and len(x_va), \"Leere Trainings/Validierungsdaten – Pfade prüfen.\"\n",
        "\n",
        "# --- Datasets erstellen ---\n",
        "train_ds = make_dataset(x_tr, y_tr, training=True)\n",
        "val_ds   = make_dataset(x_va, y_va, training=False)\n",
        "\n",
        "print(f\"Train: {len(x_tr)} | Val: {len(x_va)} | Classes: {num_classes}\")\n",
        "\n",
        "# --- Batch-Vorschau mit Textlabels ---\n",
        "imgs, labs = next(iter(train_ds.take(1)))\n",
        "imgs, labs = imgs.numpy(), labs.numpy()\n",
        "\n",
        "cols = 8\n",
        "rows = min(4, int(np.ceil(len(imgs) / cols)))\n",
        "plt.figure(figsize=(cols * 2.0, rows * 2.0), dpi=120)\n",
        "for i in range(min(len(imgs), rows * cols)):\n",
        "    ax = plt.subplot(rows, cols, i + 1)\n",
        "    ax.imshow(imgs[i]); ax.axis(\"off\")\n",
        "    li = int(labs[i])\n",
        "    title = CLASS_NAMES[li] if 0 <= li < len(CLASS_NAMES) else str(li)\n",
        "    ax.set_title(title, fontsize=8)\n",
        "plt.tight_layout(); plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vICCYEBhx6ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12) CNN-Modell definieren und kompilieren"
      ],
      "metadata": {
        "id": "glh1aNZCT_cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In dieser Zelle wird ein kompaktes Convolutional Neural Network\n",
        "# (CNN) für die GTSRB-Klassifikation erstellt und kompiliert.\n",
        "\n",
        "def build_cnn(num_classes: int):\n",
        "    # Eingabe: RGB-Bild in der vorgegebenen Zielgröße\n",
        "    inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "    # Block 1\n",
        "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
        "    x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Block 3\n",
        "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Klassifikationskopf\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs, name=\"gtsrb_cnn\")\n",
        "    return model\n",
        "\n",
        "# Modell erstellen und kompilieren\n",
        "model = build_cnn(num_classes)\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Überblick über Architektur und Parameter\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "-fbsveuGyERj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) Modelltraining mit Callbacks (EarlyStopping & Checkpoint)"
      ],
      "metadata": {
        "id": "RVZo5zJvU_8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In dieser Zelle wird das Training gestartet.\n",
        "# - EarlyStopping:   stoppt Training automatisch, wenn sich die Validierungsgenauigkeit\n",
        "#                    über mehrere Epochen nicht mehr verbessert.\n",
        "# - ModelCheckpoint: speichert das Modell mit der besten Validierungsgenauigkeit.\n",
        "\n",
        "ckpt_path = MODELS_DIR / \"gtsrb_cnn_best.keras\"\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=5,              # Wartezeit ohne Verbesserung\n",
        "        mode=\"max\",\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath=str(ckpt_path),\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,     # nur bestes Modell sichern\n",
        "        mode=\"max\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Training\n",
        "history = model.fit(train_ds,validation_data=val_ds,epochs=EPOCHS,callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "-f8WOZzgyJS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14) Trainingsverlauf: Accuracy und Loss visualisieren"
      ],
      "metadata": {
        "id": "MCkM20Zm2tyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Zelle zeigt den Verlauf von Genauigkeit (Accuracy) und\n",
        "# Verlustfunktion (Loss) für Training und Validierung über die Epochen.\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4), dpi=120)\n",
        "\n",
        "# --- Accuracy ---\n",
        "axs[0].plot(history.history['accuracy'], label='train_acc')\n",
        "axs[0].plot(history.history['val_accuracy'], label='val_acc')\n",
        "axs[0].set_title('Accuracy')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "# --- Loss ---\n",
        "axs[1].plot(history.history['loss'], label='train_loss')\n",
        "axs[1].plot(history.history['val_loss'], label='val_loss')\n",
        "axs[1].set_title('Loss')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l5dvBPyqyNxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15) Interaktive Confusion‑Matrizen (Counts & Prozent) + Zusammenfassung"
      ],
      "metadata": {
        "id": "hzpjXtqzXohM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interaktive Confusion-Matrix (Counts & Normalized) + Klassentabelle unten\n",
        "# - nutzt vorhandene y_true_list/y_pred_list, sonst wird aus val_ds (Fallback: test_ds) berechnet\n",
        "# - Achsentexte ausgeblendet (keine Namen), Klassenliste als Tabelle\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# -----------------------------\n",
        "# 1) y_true / y_pred bereitstellen\n",
        "# -----------------------------\n",
        "try:\n",
        "    # Falls bereits vorhanden (z. B. aus 19.b)\n",
        "    y_true = list(y_true_list)\n",
        "    y_pred = list(y_pred_list)\n",
        "except NameError:\n",
        "    # Neu aus Dataset berechnen\n",
        "    # Modell auswählen (reloaded > model)\n",
        "    if \"reloaded\" in globals() and reloaded is not None:\n",
        "        _model = reloaded\n",
        "    elif \"model\" in globals():\n",
        "        _model = model\n",
        "    else:\n",
        "        raise RuntimeError(\"Kein Modell gefunden (weder 'reloaded' noch 'model').\")\n",
        "\n",
        "    # Datensatz wählen: val_ds bevorzugt, sonst test_ds\n",
        "    _ds = None\n",
        "    if \"val_ds\" in globals():\n",
        "        _ds = val_ds\n",
        "    elif \"test_ds\" in globals():\n",
        "        _ds = test_ds\n",
        "    else:\n",
        "        raise RuntimeError(\"Weder 'val_ds' noch 'test_ds' verfügbar – bitte eines bereitstellen.\")\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    for xb, yb in _ds:\n",
        "        probs = _model.predict(xb, verbose=0)\n",
        "        y_pred.extend(probs.argmax(axis=1))\n",
        "        yb = yb.numpy() if hasattr(yb, \"numpy\") else yb\n",
        "        y_true.extend(yb.tolist() if hasattr(yb, \"tolist\") else list(yb))\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Confusion-Matrix (Counts & Normalized)\n",
        "# -----------------------------\n",
        "labels = list(range(num_classes))\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "row_sums = cm.sum(axis=1, keepdims=True)\n",
        "cm_norm = (cm / np.where(row_sums == 0, 1, row_sums)) * 100.0  # Prozent pro Zeile\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Klassenliste für Tabelle\n",
        "# -----------------------------\n",
        "try:\n",
        "    CLASS_NAMES  # existiert?\n",
        "    class_idx = list(range(len(CLASS_NAMES)))\n",
        "    class_names = [str(CLASS_NAMES[i]) for i in class_idx]\n",
        "except NameError:\n",
        "    class_idx = list(range(num_classes))\n",
        "    class_names = [str(i) for i in class_idx]\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Figure mit 2 Heatmaps (oben) + Tabelle (unten, über beide Spalten)\n",
        "# -----------------------------\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    specs=[[{}, {}], [{\"type\": \"table\", \"colspan\": 2}, None]],\n",
        "    subplot_titles=(\"Confusion Matrix – Counts\", \"Confusion Matrix – Normalized (%)\"),\n",
        "    vertical_spacing=0.12, horizontal_spacing=0.12\n",
        ")\n",
        "\n",
        "# (1) Counts\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=cm,\n",
        "        colorscale=\"Viridis\",\n",
        "        zmin=0, zmax=int(cm.max()) if cm.max() > 0 else 1,\n",
        "        xgap=1, ygap=1,\n",
        "        hovertemplate=\"True: %{y}<br>Pred: %{x}<br>Count: %{z}<extra></extra>\"\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# (2) Normalized (%)\n",
        "fig.add_trace(\n",
        "    go.Heatmap(\n",
        "        z=np.round(cm_norm, 1),\n",
        "        colorscale=\"Viridis\",\n",
        "        zmin=0, zmax=100,\n",
        "        xgap=1, ygap=1,\n",
        "        hovertemplate=\"True: %{y}<br>Pred: %{x}<br>%: %{z:.1f}%<extra></extra>\"\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Achsentexte ausblenden (Namen unten separat als Tabelle)\n",
        "for c in (1, 2):\n",
        "    fig.update_xaxes(showticklabels=False, title_text=\"Predicted\", row=1, col=c)\n",
        "    fig.update_yaxes(showticklabels=False, title_text=\"True\",      row=1, col=c)\n",
        "\n",
        "# (3) Tabelle: Klassenindex + Name (unten über beide Spalten)\n",
        "fig.add_trace(\n",
        "    go.Table(\n",
        "        header=dict(values=[\"Index\", \"Klassenname\"], align=\"left\"),\n",
        "        cells=dict(values=[class_idx, class_names], align=\"left\")\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    width=1400, height=900,\n",
        "    margin=dict(l=40, r=40, t=60, b=40)\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "KHwI8upfPWjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16) Classification‑Report je Klasse (interaktive Tabelle)"
      ],
      "metadata": {
        "id": "SGIlK1xcY-pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Erzeugt einen vollständigen Bericht mit Precision/Recall/F1 je Klasse\n",
        "# sowie aggregierten Kennzahlen (Accuracy, Macro/Weighted Average).\n",
        "# Darstellung als interaktive Tabelle (Plotly).\n",
        "\n",
        "# Bericht aus y_true / y_pred erzeugen\n",
        "rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "# Pro Klasse (0..num_classes-1) Zeilen aufbauen\n",
        "rows = []\n",
        "for i in range(num_classes):\n",
        "    key = str(i)\n",
        "    if key in rep:\n",
        "        rows.append({\n",
        "            \"class_id\": i,\n",
        "            \"class_name\": str(CLASS_NAMES[i]) if i < len(CLASS_NAMES) else str(i),\n",
        "            \"precision\": rep[key][\"precision\"],\n",
        "            \"recall\":    rep[key][\"recall\"],\n",
        "            \"f1-score\":  rep[key][\"f1-score\"],\n",
        "            \"support\":   int(rep[key][\"support\"]),\n",
        "        })\n",
        "\n",
        "full_df = pd.DataFrame(rows).round({\"precision\": 2, \"recall\": 2, \"f1-score\": 2})\n",
        "\n",
        "# Aggregierte Zeilen ergänzen\n",
        "acc = rep[\"accuracy\"]\n",
        "macro = rep[\"macro avg\"]\n",
        "weighted = rep[\"weighted avg\"]\n",
        "support_total = int(macro[\"support\"])\n",
        "\n",
        "avg_df = pd.DataFrame([\n",
        "    {\n",
        "        \"class_id\": \"\", \"class_name\": \"accuracy\",\n",
        "        \"precision\": round(acc, 2), \"recall\": round(acc, 2),\n",
        "        \"f1-score\": round(acc, 2), \"support\": support_total\n",
        "    },\n",
        "    {\n",
        "        \"class_id\": \"\", \"class_name\": \"macro avg\",\n",
        "        \"precision\": round(macro[\"precision\"], 2),\n",
        "        \"recall\":    round(macro[\"recall\"], 2),\n",
        "        \"f1-score\":  round(macro[\"f1-score\"], 2),\n",
        "        \"support\":   support_total\n",
        "    },\n",
        "    {\n",
        "        \"class_id\": \"\", \"class_name\": \"weighted avg\",\n",
        "        \"precision\": round(weighted[\"precision\"], 2),\n",
        "        \"recall\":    round(weighted[\"recall\"], 2),\n",
        "        \"f1-score\":  round(weighted[\"f1-score\"], 2),\n",
        "        \"support\":   support_total\n",
        "    },\n",
        "])\n",
        "\n",
        "full_df = pd.concat([full_df, avg_df], ignore_index=True)\n",
        "\n",
        "# Interaktive Tabelle anzeigen\n",
        "fig_tbl = go.Figure(data=[go.Table(\n",
        "    header=dict(values=list(full_df.columns), align=\"left\"),\n",
        "    cells=dict(values=[full_df[c] for c in full_df.columns], align=\"left\")\n",
        ")])\n",
        "fig_tbl.update_layout(width=950, height=600, title=\"Classification Report – vollständig\")\n",
        "fig_tbl.show()\n",
        "\n",
        "# Optional: als CSV speichern (zur Weitergabe/Download)\n",
        "# full_df.to_csv(\"gtsrb_classification_report_full.csv\", index=False)\n",
        "# from google.colab import files; files.download(\"gtsrb_classification_report_full.csv\")\n"
      ],
      "metadata": {
        "id": "I6fbmiyRynY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17) Testdatenauswertung und Vorhersage‑Grid"
      ],
      "metadata": {
        "id": "U_69UGAaZ1RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17) Testdatenauswertung und Vorhersage‑Grid\n",
        "# Lädt robust ein Keras‑Modell (reloaded -> model -> von Disk),\n",
        "# stellt ein Test‑Dataset zusammen (CSV/Ordner/Fallback Val),\n",
        "# führt optional eine Auswertung (loss/accuracy) durch\n",
        "# und zeigt ein Vorhersage‑Raster mit Konfidenz sowie (falls vorhanden) True‑Labels.\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import os, math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Aktives Modell ermitteln\n",
        "# -----------------------------\n",
        "def get_active_model() -> tf.keras.Model:\n",
        "    # a) Bereits geladenes Modell bevorzugen\n",
        "    if 'reloaded' in globals() and isinstance(reloaded, tf.keras.Model):\n",
        "        return reloaded\n",
        "    # b) Fallback: in-Session trainiertes Modell\n",
        "    if 'model' in globals() and isinstance(model, tf.keras.Model):\n",
        "        return model\n",
        "    # c) Von Disk laden\n",
        "    MODELS_DIR = Path(\"models\")\n",
        "    for p in [MODELS_DIR / \"gtsrb_cnn_best.keras\", MODELS_DIR / \"gtsrb_cnn_final.keras\"]:\n",
        "        if p.exists():\n",
        "            m = tf.keras.models.load_model(p)\n",
        "            globals()['reloaded'] = m  # optional verfügbar machen\n",
        "            print(\"Modell geladen:\", p)\n",
        "            return m\n",
        "    raise RuntimeError(\n",
        "        \"Kein verfügbares Modell gefunden. Weder 'reloaded' noch 'model' vorhanden \"\n",
        "        \"und keine Datei in models/gtsrb_cnn_{best,final}.keras.\"\n",
        "    )\n",
        "\n",
        "_model = get_active_model()\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Klassenliste absichern\n",
        "# -----------------------------\n",
        "try:\n",
        "    CLASS_NAMES  # existiert?\n",
        "except NameError:\n",
        "    # Aus data_info ableiten\n",
        "    CLASS_NAMES = [str(data_info['idx_to_class'][i]) for i in range(num_classes)]\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Decoder: Dateipfad -> Tensor\n",
        "# -----------------------------\n",
        "def decode_img(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    return tf.cast(img, tf.float32) / 255.0\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Pfade auflösen (relativ -> absolut)\n",
        "# -----------------------------\n",
        "candidate_roots: list[Path] = []\n",
        "for k in (\"BASE_DIR\", \"EXTRACT_DIR\"):\n",
        "    if k in globals() and isinstance(globals()[k], (str, Path)):\n",
        "        candidate_roots.append(Path(globals()[k]))\n",
        "for k in (\"train_base\", \"test_base\"):\n",
        "    if isinstance(data_info.get(k), (str, Path)):\n",
        "        candidate_roots.append(Path(data_info[k]))\n",
        "\n",
        "def resolve_path(p: str) -> Optional[str]:\n",
        "    \"\"\"Versucht, einen relativen Pfad gegen bekannte Wurzeln aufzulösen.\"\"\"\n",
        "    if not p:\n",
        "        return None\n",
        "    p = str(p)\n",
        "    # 1) wie geliefert\n",
        "    if os.path.exists(p):\n",
        "        return p\n",
        "    # 2) gegen Kandidaten\n",
        "    for root in candidate_roots:\n",
        "        cand = (root / p).as_posix()\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    # 3) häufige Unterordner‑Varianten\n",
        "    common_subs = [\"Train\", \"train\", \"images\", \"Images\", \"GTSRB/Train\", \"GTSRB\"]\n",
        "    for root in candidate_roots:\n",
        "        for sub in common_subs:\n",
        "            cand = (root / sub / p).as_posix()\n",
        "            if os.path.exists(cand):\n",
        "                return cand\n",
        "    return None\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Testquelle bestimmen und Dataset bauen\n",
        "# -----------------------------\n",
        "has_labels = True\n",
        "x_te, y_te = None, None\n",
        "\n",
        "if data_info[\"mode\"] == \"csv\" and data_info.get(\"test_df\") is not None:\n",
        "    df = data_info[\"test_df\"]\n",
        "    raw_paths = df[\"filepath\"].astype(str).tolist()\n",
        "    resolved = [resolve_path(pp) for pp in raw_paths]\n",
        "    mask = [rp is not None for rp in resolved]\n",
        "    x_te = np.array([rp for rp in resolved if rp is not None], dtype=str)\n",
        "    if \"label\" in df.columns:\n",
        "        y_te = df.loc[mask, \"label\"].to_numpy()\n",
        "    else:\n",
        "        y_te = None\n",
        "        has_labels = False\n",
        "\n",
        "elif data_info[\"mode\"] == \"dirs\" and data_info.get(\"test_base\") is not None:\n",
        "    test_base = Path(data_info[\"test_base\"])\n",
        "    paths, labels = [], []\n",
        "    for cls, idx in data_info[\"class_to_idx\"].items():\n",
        "        folder = test_base / cls\n",
        "        if not folder.exists():\n",
        "            continue\n",
        "        for ext in (\"*.png\", \"*.jpg\", \"*.jpeg\"):\n",
        "            for p in folder.rglob(ext):\n",
        "                paths.append(str(p)); labels.append(idx)\n",
        "    x_te = np.array(paths, dtype=str)\n",
        "    y_te = np.array(labels, dtype=int) if len(labels) else None\n",
        "\n",
        "# Fallback: Validation aus CSV\n",
        "if x_te is None or len(x_te) == 0:\n",
        "    print(\"Kein separates Test‑Set mit gültigen Pfaden gefunden → nutze Validation als Test.\")\n",
        "    if data_info[\"mode\"] == \"csv\":\n",
        "        df = data_info[\"val_df\"]\n",
        "        raw_paths = df[\"filepath\"].astype(str).tolist()\n",
        "        resolved = [resolve_path(pp) for pp in raw_paths]\n",
        "        mask = [rp is not None for rp in resolved]\n",
        "        x_te = np.array([rp for rp in resolved if rp is not None], dtype=str)\n",
        "        y_te = df.loc[mask, \"label\"].to_numpy()\n",
        "    else:\n",
        "        # Bei Ordner‑Variante: val_df wurde in der Pipeline erzeugt (Zelle 10)\n",
        "        try:\n",
        "            val_df  # noqa: F821  (wird erwartet)\n",
        "            raw_paths = val_df[\"filepath\"].astype(str).tolist()\n",
        "            resolved = [resolve_path(pp) for pp in raw_paths]\n",
        "            mask = [rp is not None for rp in resolved]\n",
        "            x_te = np.array([rp for rp in resolved if rp is not None], dtype=str)\n",
        "            y_te = val_df.loc[mask, \"label\"].to_numpy()\n",
        "        except Exception:\n",
        "            x_te = np.array([], dtype=str)\n",
        "\n",
        "# Letzter Fallback: direkt aus val_ds (ohne Dateipfade)\n",
        "use_pipeline_only = False\n",
        "if x_te is None or len(x_te) == 0:\n",
        "    print(\"Keine gültigen Dateipfade gefunden → nutze direkte Tensors aus val_ds für das Raster.\")\n",
        "    use_pipeline_only = True\n",
        "\n",
        "#---------------------------\n",
        "# 6) Evaluation (falls Labels) + Vorhersage‑Raster\n",
        "#---------------------------\n",
        "if not use_pipeline_only:\n",
        "    # tf.data Test‑Dataset (Dateipfade)\n",
        "    if y_te is not None:\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices((x_te, y_te))\n",
        "        test_ds = test_ds.map(lambda p, y: (decode_img(p), tf.cast(y, tf.int32)),\n",
        "                              num_parallel_calls=AUTOTUNE)\n",
        "    else:\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices(x_te)\n",
        "        test_ds = test_ds.map(lambda p: decode_img(p), num_parallel_calls=AUTOTUNE)\n",
        "    test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    # Auswertung (falls Labels vorhanden)\n",
        "    if y_te is not None:\n",
        "        loss, acc = _model.evaluate(test_ds, verbose=0)\n",
        "        print(f\"Test — loss: {loss:.4f} | accuracy: {acc:.4f}\")\n",
        "    else:\n",
        "        print(\"Testlabels nicht vorhanden → metrische Auswertung übersprungen.\")\n",
        "\n",
        "    # Vorhersage‑Raster aus Datei‑Pfaden\n",
        "    n_show = min(24, len(x_te))\n",
        "    sel_idx = np.random.choice(len(x_te), size=n_show, replace=False)\n",
        "\n",
        "    display_imgs, inputs = [], []\n",
        "    true_labels = y_te[sel_idx] if y_te is not None else None\n",
        "\n",
        "    for i in sel_idx:\n",
        "        p = x_te[i]\n",
        "        raw = tf.io.read_file(p)\n",
        "        raw = tf.image.decode_image(raw, channels=3, expand_animations=False)\n",
        "        display_imgs.append(raw.numpy().astype(\"uint8\"))\n",
        "        inp = tf.image.resize(raw, [IMG_HEIGHT, IMG_WIDTH]) / 255.0\n",
        "        inputs.append(inp.numpy())\n",
        "\n",
        "else:\n",
        "    # Vorhersage‑Raster direkt aus val_ds (ohne Pfade)\n",
        "    if \"val_ds\" not in globals():\n",
        "        raise RuntimeError(\"val_ds ist nicht verfügbar – bitte vorher erzeugen.\")\n",
        "    batches = list(val_ds.take(2))  # 1–2 Batches reichen für das Raster\n",
        "    if len(batches) == 0:\n",
        "        raise RuntimeError(\"val_ds ist leer – keine Daten für das Vorhersage‑Raster vorhanden.\")\n",
        "    imgs_list, labs_list = [], []\n",
        "    for xb, yb in batches:\n",
        "        imgs_list.append(xb.numpy())\n",
        "        labs_list.append(yb.numpy())\n",
        "    inputs = [im for arr in imgs_list for im in arr]\n",
        "    display_imgs = [(np.clip((im * 255.0), 0, 255)).astype(\"uint8\") for im in inputs]\n",
        "    true_labels = np.array([l for arr in labs_list for l in arr], dtype=int)\n",
        "\n",
        "    n_show = min(24, len(display_imgs))\n",
        "    sel_idx = np.random.choice(len(display_imgs), size=n_show, replace=False)\n",
        "    inputs = [inputs[i] for i in sel_idx]\n",
        "    display_imgs = [display_imgs[i] for i in sel_idx]\n",
        "    true_labels = true_labels[sel_idx] if true_labels is not None else None\n",
        "\n",
        "# Vorhersagen\n",
        "inputs_np = np.stack(inputs, axis=0)\n",
        "probs = _model.predict(inputs_np, verbose=0)\n",
        "pred_idx = probs.argmax(axis=1)\n",
        "pred_conf = probs.max(axis=1)\n",
        "\n",
        "# Raster zeichnen\n",
        "cols = 6\n",
        "rows = math.ceil(n_show / cols)\n",
        "plt.figure(figsize=(cols * 2.2, rows * 2.2), dpi=130)\n",
        "\n",
        "for i in range(n_show):\n",
        "    ax = plt.subplot(rows, cols, i + 1)\n",
        "    ax.imshow(display_imgs[i]); ax.axis(\"off\")\n",
        "    pname = str(CLASS_NAMES[pred_idx[i]])\n",
        "    title = f\"{pname} ({pred_conf[i]:.0%})\"\n",
        "    color = \"black\"\n",
        "    if true_labels is not None:\n",
        "        tname = str(CLASS_NAMES[int(true_labels[i])])\n",
        "        correct = (pred_idx[i] == int(true_labels[i]))\n",
        "        tick = \"✓\" if correct else \"✗\"\n",
        "        color = \"tab:green\" if correct else \"tab:red\"\n",
        "        title = f\"{tick} {pname} ({pred_conf[i]:.0%})\\ntrue: {tname}\"\n",
        "    ax.set_title(title, fontsize=8, color=color)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eHoR-DbESatB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.a) Check auf Daten-Leakage (Train vs. Val)"
      ],
      "metadata": {
        "id": "JRTfbJ0G2sH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gilt für CSV-Variante; bei Dir-Variante Pfade aus train_df/val_df nehmen\n",
        "# Diese Zelle prüft, ob es Überschneidungen (Duplikate) zwischen\n",
        "# Trainings- und Validierungsdaten gibt.\n",
        "# - Für die CSV-Variante werden die Dateipfade aus train_df und val_df\n",
        "#   gesammelt und miteinander verglichen.\n",
        "# - Falls Überschneidungen gefunden werden, deutet das auf einen\n",
        "#   fehlerhaften Split hin (Daten-Leakage), was die Accuracy verfälschen würde.\n",
        "# Ausgabe:\n",
        "#   \"Überschneidung Train∩Val: <Zahl>\"\n",
        "#   → sollte im Idealfall 0 sein.\n",
        "if data_info['mode'] == 'csv':\n",
        "    tr_paths = set(map(str, data_info['train_df']['filepath'].tolist()))\n",
        "    va_paths = set(map(str, data_info['val_df']['filepath'].tolist()))\n",
        "    inter = tr_paths & va_paths\n",
        "    print(\"Überschneidung Train∩Val:\", len(inter))\n",
        "    print(list(inter)[:5])"
      ],
      "metadata": {
        "id": "HdTdh2B2zVXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.b) Double-Check der Accuracy (Val-Set)"
      ],
      "metadata": {
        "id": "A-NPwNPH3xJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hier vergleichen wir zwei verschiedene Methoden,\n",
        "# um die Accuracy auf den Validierungsdaten zu berechnen:\n",
        "# 1) sklearn.metrics.accuracy_score\n",
        "#    - Berechnet die Accuracy auf Basis der gesammelten\n",
        "#      Vorhersagen und True-Labels aus val_ds.\n",
        "#    - Nutzt die Liste y_true_list (True-Labels) und y_pred_list (Predictions).\n",
        "# 2) model.evaluate(val_ds)\n",
        "#    - Direkter Keras-Aufruf, der Loss und Accuracy\n",
        "#      über das Dataset ausgibt.\n",
        "# Ziel:\n",
        "# - Beide Werte sollten identisch oder nahezu identisch sein.\n",
        "# - Unterschied → Hinweis auf Shuffle, Augmentierung oder falsche Labels.\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# y_true / y_pred aus val_ds sammeln (ohne Shuffle, ohne Augmentierung)\n",
        "y_true_list, y_pred_list = [], []\n",
        "for xb, yb in val_ds:\n",
        "    pb = model.predict(xb, verbose=0)\n",
        "    y_pred_list.extend(pb.argmax(axis=1))\n",
        "    y_true_list.extend(yb.numpy())\n",
        "\n",
        "acc_sklearn = accuracy_score(y_true_list, y_pred_list)\n",
        "loss_eval, acc_eval = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"Sklearn-Acc: {acc_sklearn:.4f} | model.evaluate Acc: {acc_eval:.4f}\")\n"
      ],
      "metadata": {
        "id": "z5in0_eIzVVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.c) Sicherstellen, dass das Validierungs-Dataset nicht geshuffelt ist\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H3HG2laa5A0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Für die spätere Auswertung (Confusion-Matrix, Reports, etc.)\n",
        "#   ist die exakte Reihenfolge der Samples wichtig.\n",
        "# - Falls val_ds irgendwo im Notebook mit shuffle=True gebaut wurde,\n",
        "#   könnte die Reihenfolge nicht mehr den Original-Labels entsprechen.\n",
        "# Lösung:\n",
        "# - Mit .unbatch() wird das Dataset in Einzel-Samples zerlegt.\n",
        "# - Danach mit .batch(BATCH_SIZE) wieder zusammengesetzt.\n",
        "# - Dadurch wird Shuffle rückgängig gemacht → deterministische Reihenfolge.\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)  # kein shuffle()\n"
      ],
      "metadata": {
        "id": "VNN_7R9FzbUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.d) Validierung der Spalten im val_df"
      ],
      "metadata": {
        "id": "4K79TXsS599f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Beim Laden der CSV-Daten wurden die Klassen-IDs ('ClassId')\n",
        "#   bereits auf eine neue Spalte 'label' gemappt.\n",
        "# - Alle nachfolgenden Auswertungen (Confusion-Matrix, Reports)\n",
        "#   arbeiten ausschließlich mit dieser Spalte 'label'.\n",
        "# Zweck dieser Zelle:\n",
        "# - Prüfen, ob 'val_df' wirklich die Spalte 'label' enthält.\n",
        "# - Falls nicht, Abbruch mit klarer Fehlermeldung.\n",
        "# - Zusätzlich werden alle Spaltennamen von val_df ausgegeben,\n",
        "#   damit man sofort sieht, ob ein Mapping-Fehler vorliegt.\n",
        "\n",
        "if data_info['mode'] == 'csv':\n",
        "    # Es MUSS 'label' (remapped) sein, nicht 'ClassId'\n",
        "    print(\"Spalten in val_df:\", data_info['val_df'].columns.tolist())\n",
        "    assert 'label' in data_info['val_df'].columns, \"val_df braucht die Spalte 'label'.\"\n"
      ],
      "metadata": {
        "id": "i5DXS_2nzjYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.e) Error-Matrix (Prüfung der Fehlklassifikationen)\n"
      ],
      "metadata": {
        "id": "2LUov3LRlsG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Matrix zeigt explizit nur die Fehler:\n",
        "# - Grundlage ist die Confusion-Matrix, aber die Diagonale (korrekte Treffer) wird genullt.\n",
        "# - Optional: Normierung pro Zeile (ohne Diagonale), um relative Fehlerraten in % zu sehen.\n",
        "# - So erkennt man leichter, welche Klassen am häufigsten miteinander verwechselt werden.\n",
        "# - Hinweis: Achsenbeschriftungen sind hier deaktiviert (keine Klassennamen),\n",
        "#   dafür kann man die Fehler später mit der Klassenliste zuordnen.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "cm = confusion_matrix(y_true_list, y_pred_list, labels=list(range(num_classes)))\n",
        "err = cm.copy().astype(float)\n",
        "np.fill_diagonal(err, 0)  # Diagonale nullen\n",
        "# optional: pro Zeile normieren, aber ohne Diagonale\n",
        "row_sums = cm.sum(axis=1, keepdims=True)\n",
        "err_norm = np.divide(err, np.where(row_sums==0, 1, row_sums)) * 100\n",
        "\n",
        "fig = go.Figure(go.Heatmap(\n",
        "    z=np.round(err_norm, 2),\n",
        "    colorscale=\"Inferno\", zmin=0, zmax=max(1.0, err_norm.max()),\n",
        "    hovertemplate=\"True: %{y}<br>Pred: %{x}<br>Error: %{z:.2f}%<extra></extra>\"\n",
        "))\n",
        "fig.update_layout(title=\"Error-Matrix (Zeilen-normalisiert, Diagonale=0)\", width=850, height=700)\n",
        "fig.update_xaxes(showticklabels=False); fig.update_yaxes(showticklabels=False)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "U2csrVXAz0qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.f) Confusion-Matrix mit kombinierten Werten (Counts + Prozent)"
      ],
      "metadata": {
        "id": "9yGEUWOKnxwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Variante zeigt in jeder Zelle:\n",
        "#   - die absolute Anzahl der Vorhersagen (Count)\n",
        "#   - zusätzlich den Anteil in Prozent pro Zeile (normalisiert)\n",
        "# Damit sieht man nicht nur, wie oft eine Klasse verwechselt wurde,\n",
        "# sondern auch, wie stark die Verwechslung im Verhältnis zur Klassengröße ist.\n",
        "# Klassennamen sind hier ausgeblendet (showticklabels=False),\n",
        "# können aber über die separat gelistete Klassen-Tabelle zugeordnet werden.\n",
        "cm_norm = np.divide(cm, np.where(row_sums==0, 1, row_sums)) * 100\n",
        "text = np.where(cm>0, np.char.add(cm.astype(str), np.char.add(\" | \", np.round(cm_norm,1).astype(str)+\"%\")), \"\")\n",
        "\n",
        "fig = go.Figure(go.Heatmap(\n",
        "    z=cm_norm, colorscale=\"Viridis\", zmin=0, zmax=100,\n",
        "    text=text, texttemplate=\"%{text}\", hoverinfo=\"skip\"\n",
        "))\n",
        "fig.update_layout(title=\"Confusion-Matrix (Count | % pro Zeile)\", width=1100, height=900)\n",
        "fig.update_xaxes(showticklabels=False); fig.update_yaxes(showticklabels=False)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "81PronqWz7Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.g) Fehler-Matrix (Error-Matrix) visualisieren"
      ],
      "metadata": {
        "id": "x-pukSWy2m5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Variante zeigt den relativen Fehleranteil pro Klasse (in %).\n",
        "# Dazu wird die Diagonale (korrekt klassifizierte Beispiele) genullt,\n",
        "# und anschließend je Zeile normalisiert.\n",
        "# So kannst du schnell erkennen, bei welchen Klassen die\n",
        "# meisten Fehlklassifikationen auftreten – unabhängig von\n",
        "# der absoluten Anzahl der Bilder.\n",
        "# Darstellung: Plotly Heatmap mit Rottönen, Achsen ohne Labels\n",
        "# (da bei vielen Klassen sonst zu unübersichtlich).\n",
        "\n",
        "\n",
        "err = cm.copy().astype(float)\n",
        "np.fill_diagonal(err, 0)\n",
        "row_sums = cm.sum(axis=1, keepdims=True)\n",
        "err_norm = np.divide(err, np.where(row_sums==0, 1, row_sums)) * 100\n",
        "\n",
        "fig = go.Figure(go.Heatmap(\n",
        "    z=np.round(err_norm, 2),\n",
        "    colorscale=\"Reds\", zmin=0, zmax=max(1, err_norm.max()),\n",
        "    hovertemplate=\"True: %{y}<br>Pred: %{x}<br>Error: %{z:.2f}%<extra></extra>\"\n",
        "))\n",
        "fig.update_layout(title=\"Error-Matrix (Fehleranteil je Klasse, %)\", width=850, height=750)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "mkFZcLQD1L6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18) Einzelbild‑Vorhersage mit Top‑5 Ergebnis"
      ],
      "metadata": {
        "id": "-EiB9rADaUeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lädt (falls nötig) ein gespeichertes Modell, ermöglicht die Auswahl\n",
        "# einer Bilddatei (PNG/JPG) und zeigt:\n",
        "# - links: das Originalbild mit Top‑1 Vorhersage\n",
        "# - rechts: Balkendiagramm der Top‑5 Klassen inkl. Wahrscheinlichkeiten\n",
        "\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "# Modell laden (falls 'reloaded' nicht existiert)\n",
        "try:\n",
        "    reloaded\n",
        "except NameError:\n",
        "    MODELS_DIR = Path(\"models\")\n",
        "    candidates = [\n",
        "        MODELS_DIR / \"gtsrb_cnn_best.keras\",\n",
        "        MODELS_DIR / \"gtsrb_cnn_final.keras\"\n",
        "    ]\n",
        "    reloaded = None\n",
        "    for p in candidates:\n",
        "        if p.exists():\n",
        "            reloaded = tf.keras.models.load_model(p)\n",
        "            print(\"Modell geladen:\", p)\n",
        "            break\n",
        "    if reloaded is None:\n",
        "        raise RuntimeError(\"Kein gespeichertes Modell gefunden. Bitte zuvor trainieren und speichern.\")\n",
        "\n",
        "# Klassenliste sicherstellen\n",
        "try:\n",
        "    CLASS_NAMES\n",
        "except NameError:\n",
        "    CLASS_NAMES = [data_info['idx_to_class'][i] for i in range(num_classes)]\n",
        "\n",
        "# Vorverarbeitung (falls nicht definiert)\n",
        "try:\n",
        "    load_image_for_pred\n",
        "except NameError:\n",
        "    def load_image_for_pred(path: str):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        img_resized = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH]) / 255.0\n",
        "        return img, tf.expand_dims(img_resized, axis=0)\n",
        "\n",
        "# Datei auswählen (Colab‑Upload)\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"Keine Datei ausgewählt.\")\n",
        "else:\n",
        "    img_path = next(iter(uploaded.keys()))  # hochgeladene Datei liegt lokal vor\n",
        "    raw_img, inp = load_image_for_pred(img_path)\n",
        "\n",
        "    # Vorhersage\n",
        "    probs = reloaded.predict(inp, verbose=0)[0]\n",
        "    top5_idx = np.argsort(probs)[-5:][::-1]\n",
        "    top5_probs = probs[top5_idx]\n",
        "    top5_names = [str(CLASS_NAMES[i]) for i in top5_idx]\n",
        "\n",
        "    # Darstellung: links Bild, rechts Top‑5 Balken\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5), dpi=120)\n",
        "\n",
        "    # Originalbild\n",
        "    axs[0].imshow(raw_img.numpy().astype(\"uint8\"))\n",
        "    axs[0].axis(\"off\")\n",
        "    axs[0].set_title(f\"Pred: {top5_names[0]} ({top5_probs[0]:.2%})\")\n",
        "\n",
        "    # Top‑5 Balkendiagramm (horizontal)\n",
        "    axs[1].barh(range(5)[::-1], top5_probs[::-1])\n",
        "    axs[1].set_yticks(range(5)[::-1])\n",
        "    axs[1].set_yticklabels(top5_names[::-1])\n",
        "    axs[1].set_xlim(0, 1)\n",
        "    axs[1].set_xlabel(\"Probability\")\n",
        "    for i, v in enumerate(top5_probs[::-1]):\n",
        "        axs[1].text(float(v) + 0.01, i, f\"{v:.1%}\", va=\"center\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "x6bJSRDXyzp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19) Webcam‑Erkennung (Foto aufnehmen + Top‑5 Vorhersagen)"
      ],
      "metadata": {
        "id": "-4vS4ddDbw4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nimmt in Google Colab ein Foto per Webcam auf und führt eine\n",
        "# Klassifikation durch. Darstellung: Originalbild + Top‑5 Balkendiagramm.\n",
        "# Hinweis: Funktioniert nur in Colab-Notebooks mit Webcam-Zugriff.\n",
        "\n",
        "# Modell laden (falls 'reloaded' noch nicht existiert)\n",
        "try:\n",
        "    reloaded\n",
        "except NameError:\n",
        "    MODELS_DIR = Path(\"models\")\n",
        "    reloaded = None\n",
        "    for p in [MODELS_DIR / \"gtsrb_cnn_best.keras\", MODELS_DIR / \"gtsrb_cnn_final.keras\"]:\n",
        "        if p.exists():\n",
        "            reloaded = tf.keras.models.load_model(p)\n",
        "            print(\"Modell geladen:\", p)\n",
        "            break\n",
        "    if reloaded is None:\n",
        "        raise RuntimeError(\"Kein gespeichertes Modell gefunden. Bitte zuvor trainieren und speichern.\")\n",
        "\n",
        "# Klassenliste sicherstellen\n",
        "try:\n",
        "    CLASS_NAMES\n",
        "except NameError:\n",
        "    CLASS_NAMES = [data_info['idx_to_class'][i] for i in range(num_classes)]\n",
        "\n",
        "# Vorverarbeitung konsistent zum Training\n",
        "def load_image_for_pred(path: str):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img_resized = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH]) / 255.0\n",
        "    return img, tf.expand_dims(img_resized, axis=0)\n",
        "\n",
        "# Webcam-Snapshot via JavaScript (nur Colab)\n",
        "def take_photo(filename='webcam.jpg', quality=0.9):\n",
        "    js = Javascript(\"\"\"\n",
        "      async function takePhoto(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const btn = document.createElement('button');\n",
        "        btn.textContent = 'Capture';\n",
        "        btn.style.marginTop = '8px';\n",
        "        btn.style.fontSize = '16px';\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        video.style.maxWidth = '100%';\n",
        "        div.appendChild(video);\n",
        "        div.appendChild(btn);\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "        await new Promise(resolve => btn.onclick = resolve);\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getTracks().forEach(t => t.stop());\n",
        "        div.remove();\n",
        "        return canvas.toDataURL('image/jpeg', quality);\n",
        "      }\n",
        "    \"\"\")\n",
        "    display(js)\n",
        "    data = output.eval_js(f'takePhoto({quality})')\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# 1 Bild aufnehmen → Vorhersagen berechnen\n",
        "fname = take_photo('webcam.jpg', quality=0.9)\n",
        "raw_img, inp = load_image_for_pred(fname)\n",
        "probs = reloaded.predict(inp, verbose=0)[0]\n",
        "\n",
        "# Top‑5 bestimmen\n",
        "top5_idx = np.argsort(probs)[-5:][::-1]\n",
        "top5_probs = probs[top5_idx]\n",
        "top5_names = [str(CLASS_NAMES[i]) for i in top5_idx]\n",
        "\n",
        "# Darstellung: links Foto, rechts Top‑5 Balken\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5), dpi=120)\n",
        "\n",
        "axs[0].imshow(raw_img.numpy().astype('uint8'))\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title(f\"Pred: {top5_names[0]} ({top5_probs[0]:.2%})\")\n",
        "\n",
        "axs[1].barh(range(5)[::-1], top5_probs[::-1])\n",
        "axs[1].set_yticks(range(5)[::-1])\n",
        "axs[1].set_yticklabels(top5_names[::-1])\n",
        "axs[1].set_xlim(0, 1)\n",
        "axs[1].set_xlabel(\"Probability\")\n",
        "for i, v in enumerate(top5_probs[::-1]):\n",
        "    axs[1].text(float(v) + 0.01, i, f\"{v:.1%}\", va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Gespeichert als:\", fname)\n"
      ],
      "metadata": {
        "id": "lLeJnDlHzFzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20) OpenCV: Vorverarbeitung, Einzelbild‑Vorhersage und (optionale) Live/Video‑Pipeline\n",
        "\n",
        "Dieser Abschnitt zeigt, wie ein Videoframe per OpenCV gelesen und klassifiziert wird. Für Notebooks im Browser nicht immer verfügbar."
      ],
      "metadata": {
        "id": "e9mEPgf234fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Diese Zelle stellt OpenCV‑Hilfsfunktionen bereit:\n",
        "# - preprocess_frame: Frame → RGB, Resize, Normierung, Batch‑Dimension\n",
        "# - predict_frame:    Modellvorhersage (Top‑1 Name + Konfidenz)\n",
        "# Zusätzlich sind Beispiele für Live‑Webcam und Videodatei‑Verarbeitung\n",
        "# vorbereitet (auskommentiert). In gehosteten Colab‑Umgebungen ist\n",
        "# Live‑Webcam i.d.R. nicht direkt nutzbar.\n",
        "\n",
        "import cv2\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    \"\"\"Konvertiert BGR→RGB, skaliert auf (IMG_WIDTH, IMG_HEIGHT),\n",
        "    normalisiert nach [0,1] und fügt Batch‑Achse an.\"\"\"\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return np.expand_dims(img, axis=0)\n",
        "\n",
        "def predict_frame(frame):\n",
        "    \"\"\"Gibt (Klassenname, Konfidenz) für einen einzelnen Frame zurück.\"\"\"\n",
        "    inp = preprocess_frame(frame)\n",
        "    probs = reloaded.predict(inp, verbose=0)[0]\n",
        "    idx = int(np.argmax(probs))\n",
        "    return CLASS_NAMES[idx], float(probs[idx])\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Beispiel A: Live‑Webcam (lokal, nicht für gehostetes Colab geeignet)\n",
        "# -----------------------------------------------------------------\n",
        "# cap = cv2.VideoCapture(0)\n",
        "# while True:\n",
        "#     ret, frame = cap.read()\n",
        "#     if not ret:\n",
        "#         break\n",
        "#     pred, conf = predict_frame(frame)\n",
        "#     cv2.putText(frame, f\"{pred} {conf:.0%}\", (10, 30),\n",
        "#                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "#     cv2.imshow('SignVisionAi Webcam', frame)\n",
        "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         break\n",
        "# cap.release()\n",
        "# cv2.destroyAllWindows()\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Beispiel B: Videodatei verarbeiten und annotiertes Video speichern\n",
        "# -----------------------------------------------------------------\n",
        "# in_path  = \"/content/input.mp4\"\n",
        "# out_path = \"/content/output_annot.mp4\"\n",
        "#\n",
        "# from google.colab.patches import cv2_imshow  # optional, für Inline‑Anzeige\n",
        "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "# cap = cv2.VideoCapture(in_path)\n",
        "# w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "# fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
        "# writer = cv2.VideoWriter(out_path, fourcc, fps, (w, h))\n",
        "#\n",
        "# shown = 0\n",
        "# while True:\n",
        "#     ret, frame = cap.read()\n",
        "#     if not ret:\n",
        "#         break\n",
        "#     pred, conf = predict_frame(frame)\n",
        "#     cv2.putText(frame, f\"{pred} {conf:.0%}\", (10, 30),\n",
        "#                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 0), 2)\n",
        "#     writer.write(frame)\n",
        "#     # Optional: alle 30 Frames kurz inline zeigen\n",
        "#     if shown < 5 and int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % 30 == 0:\n",
        "#         cv2_imshow(frame); shown += 1\n",
        "#\n",
        "# cap.release()\n",
        "# writer.release()\n",
        "# print(\"Gespeichert:\", out_path)\n"
      ],
      "metadata": {
        "id": "UHXUleRHzMc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.a) — Modell lokal herunterladen"
      ],
      "metadata": {
        "id": "f-UldZAMqK-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "save_path = \"SignVisionGTSRB_model.keras\"\n",
        "model.save(save_path)\n",
        "files.download(save_path)\n"
      ],
      "metadata": {
        "id": "N4AT-ejhnlQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.b) — Modell im Projektordner sichern"
      ],
      "metadata": {
        "id": "8gJg8RLuoJ9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras-Version (Modern - empfohlen für TF >= 2.11)\n",
        "keras_path = MODELS_DIR / \"SignVisionGTSRB_model.keras\"\n",
        "model.save(keras_path)\n",
        "print(f\"Modell im modernen Keras-Format gespeichert: {keras_path}\")\n",
        "\n",
        "# HDF5-Version (kompatibel mit älteren Keras/TensorFlow-Versionen)\n",
        "h5_path = MODELS_DIR / \"SignVisionGTSRB_model.h5\"\n",
        "model.save(h5_path)\n",
        "print(f\"Modell im HDF5-Format gespeichert: {h5_path}\")\n"
      ],
      "metadata": {
        "id": "b81UsykboHAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.c) — Modell in Google Drive speichern"
      ],
      "metadata": {
        "id": "GzHn4fzonrXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/SignVisionGTSRB_model.keras\"\n",
        "model.save(save_path)\n",
        "print(\"Modell gespeichert in Google Drive:\", save_path)"
      ],
      "metadata": {
        "id": "AznB_v_Pn2kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22) — Modell laden"
      ],
      "metadata": {
        "id": "X2ZOoGedoc0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "reloaded = None\n",
        "candidates = [\n",
        "    MODELS_DIR / \"SignVisionGTSRB_model.keras\",   # bevorzugt\n",
        "    MODELS_DIR / \"SignVisionGTSRB_model.h5\"       # Fallback\n",
        "]\n",
        "\n",
        "for p in candidates:\n",
        "    if Path(p).exists():\n",
        "        reloaded = tf.keras.models.load_model(p)\n",
        "        print(\"Modell geladen:\", p)\n",
        "        break\n",
        "\n",
        "if reloaded is None:\n",
        "    print(\"Kein gespeichertes Modell gefunden. Bitte zuerst trainieren und speichern.\")\n"
      ],
      "metadata": {
        "id": "Rj9aeem8oght"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23) Auswertung des geladenen Modells"
      ],
      "metadata": {
        "id": "DGrFdWHXp_VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Führt eine kurze Auswertung auf den Validierungsdaten (val_ds) durch,\n",
        "# um das geladene Modell zu überprüfen.\n",
        "\n",
        "if reloaded is not None:\n",
        "    loss, acc = reloaded.evaluate(val_ds, verbose=0)\n",
        "    print(f\"Validation — Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
        "else:\n",
        "    print(\"Kein Modell geladen — bitte zuerst Zelle 24 ausführen.\")\n"
      ],
      "metadata": {
        "id": "1HJoq4nLp86C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}